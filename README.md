# DeepPTM: Protein Post-translational Modification Prediction from Protein Sequences by Combining Deep Protein Language Model with Vision Transformers

DeepPTM: Protein Post-translational Modification Prediction from Protein Sequences by Combining Deep Protein Language Model with Vision Transformers

Prepared by Necla Nisa Soylu and Emre Sefer

We have obtained protein sequences and the corresponding post-translational modifications from CPLM database (http://cplm.biocuckoo.cn/) and focused on 4 important post-translational modifications over lysine in our experiments: Succinylation, Glutarylation, Crotonylation, and Glycation. Even though our main focus was to predict the modification sites on Homo sapiens, we also tested DeepPtm on Mus musculus and Saccharomyces cerevisiae. List of the datasets are given below:

- Succinylation (Homo Sapiens)
- Succinylation (Mus musculus)
- Succinylation (S. cerevisiae)
- Ubiquitination (Homo Sapiens)
- Crotonylation (Homo Sapiens)
- Glycation (Homo Sapiens)

We have trained the baseline machine learning approaches by converting each amino acid into numeric values via one-hot encoding strategy and each of the 20 amino acid is mapped from 20Ã—1 vectors [1, 0, . . . , 0] to [0, 0, . . . , 1] respectively. As a result, we have obtained vectors with lengths 420, for each 21 nucleotide long protein sequence. We used Random Forest (RF) and Extreme Gradient Boosting (XGBoost) as our baseline models by obtaining optimal model parameters with using hyperparameter tuning. 

After comparing model performance by using different sequence legths such as 15, 21, 27 and 33, proteins were converted into peptides with a fixed length of 21, where lysine is at the central residue (modification either occurs or not), and 10 residues exist at each upstream and downstream.  

Since DEEPPTM performs the best with 40% CD-HIT threshold compared to applying 30% threshold and without removing any sequences, we decided to proceed with 40% CD-HIT threshold to remove redundancy in homologous sequences. Detailed information about CD-HIT tool can be find in the following website: https://sites.google.com/view/cd-hit

 You can find data preprocessing codes and our datasets in the following files:

- processdata.py
- Suc_Human_cdhit40_Equal.xlsx
- Suc_Mouse_cdhit40_Equal.xlsx
- Suc_yeast_cdhit40_Equal.xlsx
- Ubi_Human_dhit40_Equal.xlsx
- Crot_Human_cdhit40_Equal.xlsx
- Gly_Human_dhit40_Equal.xlsx

In order to find the optimal threshold value for our model, new datasets were generated by using different CD-HIT threshold values after fixing the sequence length as 21. For example based on Homo Sapiens Succinylation dataset after applying 30%, 40% threshold values and without removing any sequences we got following datasets:

- Succinylation_human_cdhit30_equal.xlsx
- Succinylation_human_cdhit40_equal.xlsx
- Succinylation_human_cdhit100_equal.xlsx

In order to find the optimal sequence length for our model, different datasets with different sequence lenghts were created. For example based on Homo Sapiens Succinylation dataset we generated sequence lengths as 15, 21,27 and 33.

- Succinylation_human_cdhit40_equal_seq_15.xlsx
- Succinylation_human_cdhit40_equal.xlsx
- Succinylation_human_cdhit40_equal_seq_27.xlsx
- Succinylation_human_cdhit40_equal_seq_33.xlsx

In terms of deep learning approaches, each amino acids were considered as a single word in human language and corresponding vector embeddings were created by using one of the well-known natural language processing model BERT (Bidirectional Encoder Representations from Transformers). Following files contain the vector creation process for each peptides:

- BERT_Vector_Creation.ipynb

Then we compared the performance of BERT + Random Forest, BERT + XGBoost, BERT + 1D CNN, BERT + 2D CNN, BERT + ViT. Related codes can be found in the following file:

- BERT_1D_2D_CNN_and_ViT.ipynb

Instead of BERT, we designed our architecture by using protein language model ProtBERT. The number of layers in ProtBERT has increased to 30, instead of 12 as in the original BERT architecture. ProtBERT can infer functional attributes over smaller sequences while running a larger batch size, which leads to training on longer sequences to become more efficient. In our case, we trained our model by using only the last layer for ProtBERT, since there is no major performance difference between using the last 4 layers (as it was implemented while vector embedding generation using BERT) and using only the last layer for ProtBERT. 
You can find our codes in the following files for the proposed method DeepPTM: ProtBERT + ViT:

- ProtBERT_Vector_Creation.ipynb
- ProtBERT_1D_2D_CNN_and_ViT.ipynb


Cross species and cross-modification were also implemented and can be found in the following file:

- Cross-Species for Succinylation and Cross-Modification.ipynb

You can install the required dependencies:

pip install -r requirements.txt