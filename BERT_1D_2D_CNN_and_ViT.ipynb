{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1h0aQdQhbHY"
   },
   "source": [
    "Here, choose embeddings file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XRmiQ7J0LIE8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.load(\"embeddings/bert_21_homo_ubi_cdhit_40_embeddings_X.npy\")\n",
    "#x = np.load(\"embeddings/bert_21_suc_suc_cdhit_40_embeddings_X.npy\")\n",
    "#x = np.load(\"embeddings/bert_21_mus_suc_cdhit_40_embeddings_X.npy\")\n",
    "#x = np.load(\"embeddings/bert_21_homo_suc_cdhit_40_embeddings_X.npy\")\n",
    "#x = np.load(\"embeddings/bert_21_homo_gly_cdhit_40_embeddings_X.npy\")\n",
    "#x = np.load(\"embeddings/bert_21_homo_crot_cdhit_40_embeddings_X.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hBmCAMwhj_R"
   },
   "source": [
    "Here, choose label file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LiqYjD-qLXB_"
   },
   "outputs": [],
   "source": [
    "y = np.load(\"embeddings/bert_21_homo_ubi_cdhit_40_embeddings_Y.npy\")\n",
    "#y = np.load(\"embeddings/bert_21_suc_suc_cdhit_40_embeddings_Y.npy\")\n",
    "#y = np.load(\"embeddings/bert_21_mus_suc_cdhit_40_embeddings_Y.npy\")\n",
    "#y = np.load(\"embeddings/bert_21_homo_suc_cdhit_40_embeddings_Y.npy\")\n",
    "#y = np.load(\"embeddings/bert_21_homo_gly_cdhit_40_embeddings_Y.npy\")\n",
    "#y = np.load(\"embeddings/bert_21_homo_crot_cdhit_40_embeddings_Y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wkyDmq8Kyl2b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "x_dataframe = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "A7L45Ff2y_4f",
    "outputId": "1cc9310f-c5d4-4a67-aad4-dde0944cdd18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17654</th>\n",
       "      <th>17655</th>\n",
       "      <th>17656</th>\n",
       "      <th>17657</th>\n",
       "      <th>17658</th>\n",
       "      <th>17659</th>\n",
       "      <th>17660</th>\n",
       "      <th>17661</th>\n",
       "      <th>17662</th>\n",
       "      <th>17663</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.782213</td>\n",
       "      <td>-0.124671</td>\n",
       "      <td>-0.141697</td>\n",
       "      <td>0.176971</td>\n",
       "      <td>-0.442486</td>\n",
       "      <td>-0.064883</td>\n",
       "      <td>0.649165</td>\n",
       "      <td>0.266144</td>\n",
       "      <td>-0.155975</td>\n",
       "      <td>-0.370200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031379</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>-0.161234</td>\n",
       "      <td>-0.262236</td>\n",
       "      <td>0.025050</td>\n",
       "      <td>-0.082000</td>\n",
       "      <td>-0.028757</td>\n",
       "      <td>0.067166</td>\n",
       "      <td>-0.265681</td>\n",
       "      <td>-0.092071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.319376</td>\n",
       "      <td>1.104472</td>\n",
       "      <td>-1.257744</td>\n",
       "      <td>-0.986338</td>\n",
       "      <td>-0.664544</td>\n",
       "      <td>-0.130991</td>\n",
       "      <td>1.246550</td>\n",
       "      <td>-0.464422</td>\n",
       "      <td>-0.302342</td>\n",
       "      <td>-0.867527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094962</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.078585</td>\n",
       "      <td>-0.130398</td>\n",
       "      <td>0.396239</td>\n",
       "      <td>-0.295938</td>\n",
       "      <td>-0.349839</td>\n",
       "      <td>0.105124</td>\n",
       "      <td>-0.236944</td>\n",
       "      <td>0.571456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.904047</td>\n",
       "      <td>-0.229957</td>\n",
       "      <td>-0.156957</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>-0.584103</td>\n",
       "      <td>0.052987</td>\n",
       "      <td>0.524052</td>\n",
       "      <td>0.443856</td>\n",
       "      <td>-0.203538</td>\n",
       "      <td>-0.283925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083664</td>\n",
       "      <td>0.013870</td>\n",
       "      <td>-0.151100</td>\n",
       "      <td>-0.243960</td>\n",
       "      <td>0.012364</td>\n",
       "      <td>-0.060610</td>\n",
       "      <td>-0.048818</td>\n",
       "      <td>0.060473</td>\n",
       "      <td>-0.241524</td>\n",
       "      <td>-0.111876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.080645</td>\n",
       "      <td>0.867144</td>\n",
       "      <td>-0.856271</td>\n",
       "      <td>-1.009194</td>\n",
       "      <td>-0.758203</td>\n",
       "      <td>0.091213</td>\n",
       "      <td>0.758180</td>\n",
       "      <td>-0.656531</td>\n",
       "      <td>0.412248</td>\n",
       "      <td>0.546839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011425</td>\n",
       "      <td>0.197645</td>\n",
       "      <td>0.091389</td>\n",
       "      <td>-0.246717</td>\n",
       "      <td>0.365010</td>\n",
       "      <td>-0.236992</td>\n",
       "      <td>-0.225534</td>\n",
       "      <td>0.142884</td>\n",
       "      <td>-0.033701</td>\n",
       "      <td>0.634907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.061502</td>\n",
       "      <td>-0.368256</td>\n",
       "      <td>-0.190613</td>\n",
       "      <td>0.019378</td>\n",
       "      <td>-0.253407</td>\n",
       "      <td>0.050135</td>\n",
       "      <td>0.479626</td>\n",
       "      <td>0.392251</td>\n",
       "      <td>-0.335405</td>\n",
       "      <td>-0.304329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041791</td>\n",
       "      <td>0.019136</td>\n",
       "      <td>-0.168744</td>\n",
       "      <td>-0.211274</td>\n",
       "      <td>0.043320</td>\n",
       "      <td>-0.047533</td>\n",
       "      <td>-0.014311</td>\n",
       "      <td>0.072769</td>\n",
       "      <td>-0.225191</td>\n",
       "      <td>-0.106579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>-0.162566</td>\n",
       "      <td>0.876729</td>\n",
       "      <td>-1.058215</td>\n",
       "      <td>-1.321009</td>\n",
       "      <td>-1.241336</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>1.050082</td>\n",
       "      <td>-0.278553</td>\n",
       "      <td>-0.271535</td>\n",
       "      <td>0.042179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049625</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.085361</td>\n",
       "      <td>0.111224</td>\n",
       "      <td>0.466310</td>\n",
       "      <td>-0.328403</td>\n",
       "      <td>-0.268423</td>\n",
       "      <td>0.154869</td>\n",
       "      <td>-0.412042</td>\n",
       "      <td>0.472309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>-0.656452</td>\n",
       "      <td>-0.281502</td>\n",
       "      <td>-0.145550</td>\n",
       "      <td>-0.186631</td>\n",
       "      <td>-0.320786</td>\n",
       "      <td>-0.005594</td>\n",
       "      <td>0.409153</td>\n",
       "      <td>0.315339</td>\n",
       "      <td>-0.052799</td>\n",
       "      <td>-0.346843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050526</td>\n",
       "      <td>0.013067</td>\n",
       "      <td>-0.172364</td>\n",
       "      <td>-0.195747</td>\n",
       "      <td>0.044481</td>\n",
       "      <td>-0.075038</td>\n",
       "      <td>-0.033817</td>\n",
       "      <td>0.060396</td>\n",
       "      <td>-0.270067</td>\n",
       "      <td>-0.066649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.046346</td>\n",
       "      <td>0.838802</td>\n",
       "      <td>-1.047061</td>\n",
       "      <td>-1.139280</td>\n",
       "      <td>-0.841509</td>\n",
       "      <td>0.024932</td>\n",
       "      <td>0.925196</td>\n",
       "      <td>-0.593241</td>\n",
       "      <td>0.133542</td>\n",
       "      <td>0.385614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207397</td>\n",
       "      <td>0.233712</td>\n",
       "      <td>0.066569</td>\n",
       "      <td>-0.160081</td>\n",
       "      <td>0.733986</td>\n",
       "      <td>-0.204189</td>\n",
       "      <td>-0.318482</td>\n",
       "      <td>0.306679</td>\n",
       "      <td>-0.173608</td>\n",
       "      <td>0.978289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>-0.903575</td>\n",
       "      <td>-0.279092</td>\n",
       "      <td>-0.183276</td>\n",
       "      <td>0.085107</td>\n",
       "      <td>-0.344833</td>\n",
       "      <td>0.073812</td>\n",
       "      <td>0.608878</td>\n",
       "      <td>0.217553</td>\n",
       "      <td>-0.058670</td>\n",
       "      <td>-0.469833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099563</td>\n",
       "      <td>-0.005592</td>\n",
       "      <td>-0.192161</td>\n",
       "      <td>-0.244181</td>\n",
       "      <td>0.009522</td>\n",
       "      <td>-0.088248</td>\n",
       "      <td>-0.023222</td>\n",
       "      <td>0.038602</td>\n",
       "      <td>-0.293412</td>\n",
       "      <td>-0.047889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>-0.072946</td>\n",
       "      <td>0.773582</td>\n",
       "      <td>-0.863557</td>\n",
       "      <td>-1.186181</td>\n",
       "      <td>-0.674268</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.845423</td>\n",
       "      <td>-0.453220</td>\n",
       "      <td>0.394613</td>\n",
       "      <td>0.564087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243863</td>\n",
       "      <td>0.194469</td>\n",
       "      <td>0.087064</td>\n",
       "      <td>-0.244831</td>\n",
       "      <td>0.550052</td>\n",
       "      <td>-0.277002</td>\n",
       "      <td>-0.239155</td>\n",
       "      <td>0.293468</td>\n",
       "      <td>-0.010493</td>\n",
       "      <td>0.768589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 17664 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6      \\\n",
       "0    -0.782213 -0.124671 -0.141697  0.176971 -0.442486 -0.064883  0.649165   \n",
       "1     0.319376  1.104472 -1.257744 -0.986338 -0.664544 -0.130991  1.246550   \n",
       "2    -0.904047 -0.229957 -0.156957  0.041050 -0.584103  0.052987  0.524052   \n",
       "3    -0.080645  0.867144 -0.856271 -1.009194 -0.758203  0.091213  0.758180   \n",
       "4    -1.061502 -0.368256 -0.190613  0.019378 -0.253407  0.050135  0.479626   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3995 -0.162566  0.876729 -1.058215 -1.321009 -1.241336  0.095000  1.050082   \n",
       "3996 -0.656452 -0.281502 -0.145550 -0.186631 -0.320786 -0.005594  0.409153   \n",
       "3997  0.046346  0.838802 -1.047061 -1.139280 -0.841509  0.024932  0.925196   \n",
       "3998 -0.903575 -0.279092 -0.183276  0.085107 -0.344833  0.073812  0.608878   \n",
       "3999 -0.072946  0.773582 -0.863557 -1.186181 -0.674268  0.009662  0.845423   \n",
       "\n",
       "         7         8         9      ...     17654     17655     17656  \\\n",
       "0     0.266144 -0.155975 -0.370200  ...  0.031379  0.001916 -0.161234   \n",
       "1    -0.464422 -0.302342 -0.867527  ...  0.094962  0.145508  0.078585   \n",
       "2     0.443856 -0.203538 -0.283925  ...  0.083664  0.013870 -0.151100   \n",
       "3    -0.656531  0.412248  0.546839  ...  0.011425  0.197645  0.091389   \n",
       "4     0.392251 -0.335405 -0.304329  ...  0.041791  0.019136 -0.168744   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3995 -0.278553 -0.271535  0.042179  ... -0.049625  0.010590  0.085361   \n",
       "3996  0.315339 -0.052799 -0.346843  ...  0.050526  0.013067 -0.172364   \n",
       "3997 -0.593241  0.133542  0.385614  ... -0.207397  0.233712  0.066569   \n",
       "3998  0.217553 -0.058670 -0.469833  ...  0.099563 -0.005592 -0.192161   \n",
       "3999 -0.453220  0.394613  0.564087  ... -0.243863  0.194469  0.087064   \n",
       "\n",
       "         17657     17658     17659     17660     17661     17662     17663  \n",
       "0    -0.262236  0.025050 -0.082000 -0.028757  0.067166 -0.265681 -0.092071  \n",
       "1    -0.130398  0.396239 -0.295938 -0.349839  0.105124 -0.236944  0.571456  \n",
       "2    -0.243960  0.012364 -0.060610 -0.048818  0.060473 -0.241524 -0.111876  \n",
       "3    -0.246717  0.365010 -0.236992 -0.225534  0.142884 -0.033701  0.634907  \n",
       "4    -0.211274  0.043320 -0.047533 -0.014311  0.072769 -0.225191 -0.106579  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3995  0.111224  0.466310 -0.328403 -0.268423  0.154869 -0.412042  0.472309  \n",
       "3996 -0.195747  0.044481 -0.075038 -0.033817  0.060396 -0.270067 -0.066649  \n",
       "3997 -0.160081  0.733986 -0.204189 -0.318482  0.306679 -0.173608  0.978289  \n",
       "3998 -0.244181  0.009522 -0.088248 -0.023222  0.038602 -0.293412 -0.047889  \n",
       "3999 -0.244831  0.550052 -0.277002 -0.239155  0.293468 -0.010493  0.768589  \n",
       "\n",
       "[4000 rows x 17664 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJswL9-OzNhz",
    "outputId": "24f69f9e-9581-4c99-a19a-1a556360d3dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlPVGWWyUpuL"
   },
   "source": [
    "# 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pcREeCPUn50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9t/frr8qwnd5h1_3mymt9gwrcm40000gn/T/ipykernel_83782/4245958795.py:12: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import RandomSearch\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.8296 - accuracy: 0.5253 - precision: 0.5253 - recall: 0.5253 - val_loss: 0.6964 - val_accuracy: 0.5250 - val_precision: 0.5250 - val_recall: 0.5250\n",
      "Epoch 2/50\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.6867 - accuracy: 0.5441 - precision: 0.5441 - recall: 0.5441 - val_loss: 0.6933 - val_accuracy: 0.4975 - val_precision: 0.4975 - val_recall: 0.4975\n",
      "Epoch 3/50\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.6744 - accuracy: 0.5781 - precision: 0.5781 - recall: 0.5781 - val_loss: 0.6881 - val_accuracy: 0.5350 - val_precision: 0.5350 - val_recall: 0.5350\n",
      "Epoch 4/50\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.6678 - accuracy: 0.5962 - precision: 0.5962 - recall: 0.5962 - val_loss: 0.7247 - val_accuracy: 0.5325 - val_precision: 0.5325 - val_recall: 0.5325\n",
      "Epoch 5/50\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.6550 - accuracy: 0.6153 - precision: 0.6153 - recall: 0.6153 - val_loss: 0.6884 - val_accuracy: 0.5500 - val_precision: 0.5500 - val_recall: 0.5500\n",
      "Epoch 6/50\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.6401 - accuracy: 0.6372 - precision: 0.6372 - recall: 0.6372 - val_loss: 0.6794 - val_accuracy: 0.5575 - val_precision: 0.5575 - val_recall: 0.5575\n",
      "Epoch 7/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.6266 - accuracy: 0.6522 - precision: 0.6522 - recall: 0.6522 - val_loss: 0.6700 - val_accuracy: 0.5675 - val_precision: 0.5675 - val_recall: 0.5675\n",
      "Epoch 8/50\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.6110 - accuracy: 0.6625 - precision: 0.6625 - recall: 0.6625 - val_loss: 0.7444 - val_accuracy: 0.5425 - val_precision: 0.5425 - val_recall: 0.5425\n",
      "Epoch 9/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.6064 - accuracy: 0.6612 - precision: 0.6612 - recall: 0.6612 - val_loss: 0.6689 - val_accuracy: 0.5825 - val_precision: 0.5825 - val_recall: 0.5825\n",
      "Epoch 10/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.5862 - accuracy: 0.6853 - precision: 0.6853 - recall: 0.6853 - val_loss: 0.6968 - val_accuracy: 0.5725 - val_precision: 0.5725 - val_recall: 0.5725\n",
      "Epoch 11/50\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.5764 - accuracy: 0.6947 - precision: 0.6947 - recall: 0.6947 - val_loss: 0.7466 - val_accuracy: 0.5700 - val_precision: 0.5700 - val_recall: 0.5700\n",
      "Epoch 12/50\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 0.5679 - accuracy: 0.6966 - precision: 0.6966 - recall: 0.6966 - val_loss: 0.6808 - val_accuracy: 0.5900 - val_precision: 0.5900 - val_recall: 0.5900\n",
      "Epoch 13/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.5392 - accuracy: 0.7169 - precision: 0.7169 - recall: 0.7169 - val_loss: 0.7562 - val_accuracy: 0.5700 - val_precision: 0.5700 - val_recall: 0.5700\n",
      "Epoch 14/50\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.5264 - accuracy: 0.7244 - precision: 0.7244 - recall: 0.7244 - val_loss: 0.7056 - val_accuracy: 0.6025 - val_precision: 0.6025 - val_recall: 0.6025\n",
      "Epoch 15/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.5140 - accuracy: 0.7337 - precision: 0.7337 - recall: 0.7337 - val_loss: 0.7717 - val_accuracy: 0.5800 - val_precision: 0.5800 - val_recall: 0.5800\n",
      "Epoch 16/50\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.5030 - accuracy: 0.7484 - precision: 0.7484 - recall: 0.7484 - val_loss: 0.7215 - val_accuracy: 0.5975 - val_precision: 0.5975 - val_recall: 0.5975\n",
      "Epoch 17/50\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.4959 - accuracy: 0.7566 - precision: 0.7566 - recall: 0.7566 - val_loss: 0.7401 - val_accuracy: 0.6000 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 18/50\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.4829 - accuracy: 0.7700 - precision: 0.7700 - recall: 0.7700 - val_loss: 0.7842 - val_accuracy: 0.6025 - val_precision: 0.6025 - val_recall: 0.6025\n",
      "Epoch 19/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.4838 - accuracy: 0.7644 - precision: 0.7644 - recall: 0.7644 - val_loss: 0.7394 - val_accuracy: 0.5950 - val_precision: 0.5950 - val_recall: 0.5950\n",
      "Epoch 20/50\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.4356 - accuracy: 0.7903 - precision: 0.7903 - recall: 0.7903 - val_loss: 0.8414 - val_accuracy: 0.5925 - val_precision: 0.5925 - val_recall: 0.5925\n",
      "Epoch 21/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.4394 - accuracy: 0.7903 - precision: 0.7903 - recall: 0.7903 - val_loss: 0.7790 - val_accuracy: 0.5775 - val_precision: 0.5775 - val_recall: 0.5775\n",
      "Epoch 22/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.4354 - accuracy: 0.8003 - precision: 0.8003 - recall: 0.8003 - val_loss: 0.7721 - val_accuracy: 0.6000 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 23/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.4013 - accuracy: 0.8103 - precision: 0.8103 - recall: 0.8103 - val_loss: 0.8305 - val_accuracy: 0.5900 - val_precision: 0.5900 - val_recall: 0.5900\n",
      "Epoch 24/50\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.4095 - accuracy: 0.8009 - precision: 0.8009 - recall: 0.8009 - val_loss: 0.8321 - val_accuracy: 0.5800 - val_precision: 0.5800 - val_recall: 0.5800\n",
      "Epoch 25/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.4099 - accuracy: 0.8037 - precision: 0.8037 - recall: 0.8037 - val_loss: 0.7986 - val_accuracy: 0.5975 - val_precision: 0.5975 - val_recall: 0.5975\n",
      "Epoch 26/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.3810 - accuracy: 0.8234 - precision: 0.8234 - recall: 0.8234 - val_loss: 0.8831 - val_accuracy: 0.5975 - val_precision: 0.5975 - val_recall: 0.5975\n",
      "Epoch 27/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.3709 - accuracy: 0.8319 - precision: 0.8319 - recall: 0.8319 - val_loss: 0.9099 - val_accuracy: 0.5750 - val_precision: 0.5750 - val_recall: 0.5750\n",
      "Epoch 28/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.3742 - accuracy: 0.8241 - precision: 0.8241 - recall: 0.8241 - val_loss: 0.8436 - val_accuracy: 0.5750 - val_precision: 0.5750 - val_recall: 0.5750\n",
      "Epoch 29/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.3709 - accuracy: 0.8341 - precision: 0.8341 - recall: 0.8341 - val_loss: 0.9075 - val_accuracy: 0.5950 - val_precision: 0.5950 - val_recall: 0.5950\n",
      "Epoch 30/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.3495 - accuracy: 0.8359 - precision: 0.8359 - recall: 0.8359 - val_loss: 0.9345 - val_accuracy: 0.5575 - val_precision: 0.5575 - val_recall: 0.5575\n",
      "Epoch 31/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.3533 - accuracy: 0.8459 - precision: 0.8459 - recall: 0.8459 - val_loss: 0.9354 - val_accuracy: 0.5775 - val_precision: 0.5775 - val_recall: 0.5775\n",
      "Epoch 32/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.3367 - accuracy: 0.8491 - precision: 0.8491 - recall: 0.8491 - val_loss: 0.9343 - val_accuracy: 0.5625 - val_precision: 0.5625 - val_recall: 0.5625\n",
      "Epoch 33/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.3225 - accuracy: 0.8534 - precision: 0.8534 - recall: 0.8534 - val_loss: 0.9661 - val_accuracy: 0.5600 - val_precision: 0.5600 - val_recall: 0.5600\n",
      "Epoch 34/50\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.3397 - accuracy: 0.8516 - precision: 0.8516 - recall: 0.8516 - val_loss: 1.0043 - val_accuracy: 0.5500 - val_precision: 0.5500 - val_recall: 0.5500\n",
      "Epoch 35/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.3147 - accuracy: 0.8612 - precision: 0.8612 - recall: 0.8612 - val_loss: 0.9736 - val_accuracy: 0.5750 - val_precision: 0.5750 - val_recall: 0.5750\n",
      "Epoch 36/50\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.3166 - accuracy: 0.8603 - precision: 0.8603 - recall: 0.8603 - val_loss: 0.9176 - val_accuracy: 0.5850 - val_precision: 0.5850 - val_recall: 0.5850\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 3s 20ms/step - loss: 0.3035 - accuracy: 0.8666 - precision: 0.8666 - recall: 0.8666 - val_loss: 0.9892 - val_accuracy: 0.5600 - val_precision: 0.5600 - val_recall: 0.5600\n",
      "Epoch 38/50\n",
      " 13/160 [=>............................] - ETA: 2s - loss: 0.2637 - accuracy: 0.8923 - precision: 0.8923 - recall: 0.8923"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv1D, ZeroPadding1D, MaxPooling1D, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import utils\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from kerastuner import RandomSearch\n",
    "from tensorflow.keras import utils\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.20, random_state = 42)\n",
    "xval, xtest, yval, ytest = train_test_split(xtest, ytest, test_size = 0.5)\n",
    "\n",
    "def CNN_1D():\n",
    "    model = Sequential()\n",
    "\n",
    "    # layer 1\n",
    "    model.add(Conv1D(8, 4, input_shape=(23*768, 1), activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    # layer 2\n",
    "    model.add(Conv1D(8, 2, activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Flattening Layer:\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "    # Last Layer:\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr = 0.00005),\n",
    "                  metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    return model\n",
    "\n",
    "model1 = CNN_1D()\n",
    "\n",
    "a = np.asarray(xtrain).reshape(len(np.asarray(xtrain)),23*768,1)\n",
    "\n",
    "history1_ = model1.fit(np.asarray(xtrain).reshape(len(np.asarray(xtrain)),23*768,1), utils.to_categorical(ytrain,2),\n",
    "                    validation_data=(np.asarray(xval).reshape(len(np.asarray(xval)),23*768,1), utils.to_categorical(yval,2)),\n",
    "                    epochs=50, batch_size=20, verbose=1)\n",
    "\n",
    "score = model1.evaluate(np.asarray(xtest).reshape(len(np.asarray(xtest)),23*768,1), utils.to_categorical(ytest,2), verbose=1)\n",
    "print(\"categorical_crossentropy loss, Accuracy, Precision, Recall scores:\", score)\n",
    "\n",
    "\n",
    "plt.plot(history1_.history[\"accuracy\"])\n",
    "plt.plot(history1_.history[\"val_accuracy\"])\n",
    "plt.xlabel(\"Epoch\", fontsize = 15)\n",
    "plt.ylabel(\"True positive rate\", fontsize = 15)\n",
    "plt.title(\"1D CNN Model Accuracy\", fontsize = 20)\n",
    "plt.legend([\"Train\", \"Test\"], loc=\"upper left\",  prop={\"size\": 15})\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10), dpi = 600)\n",
    "\n",
    "plt.plot(history1_.history[\"loss\"])\n",
    "plt.plot(history1_.history[\"val_loss\"])\n",
    "plt.xlabel(\"Epoch\", fontsize = 15)\n",
    "plt.ylabel(\"Loss\", fontsize = 15)\n",
    "plt.title(\"1D CNN Model Loss\", fontsize = 20)\n",
    "plt.legend([\"Train\", \"Test\"], loc=\"upper left\",  prop={\"size\": 15})\n",
    "plt.show()\n",
    "\n",
    "print(model1.summary())\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnn_predictions1 = model1.predict(xtest)\n",
    "bert2_probs_cnn1 = cnn_predictions1[:,1]\n",
    "cnn_predictions1 = np.argmax(cnn_predictions1, axis = 1)\n",
    "confusion_matrix = confusion_matrix(ytest, cnn_predictions1)\n",
    "sns.heatmap(confusion_matrix, annot = True, fmt = \"d\", cbar = False)\n",
    "plt.title(\"BERT + 1D CNN Confusion Matrix\", fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_keras_bert_cnn1_hyp, tpr_keras_bert_cnn1_hyp, _ = roc_curve(ytest, bert2_probs_cnn1)\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "auc_keras_bert_cnn1_hyp = auc(fpr_keras_bert_cnn1_hyp, tpr_keras_bert_cnn1_hyp)\n",
    "print(\"AUC Score\", auc_keras_bert_cnn1_hyp)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras_bert_cnn1_hyp, tpr_keras_bert_cnn1_hyp, label=\"BERT + 1D-CNN: {:.3f}\".format(auc_keras_bert_cnn1_hyp))\n",
    "plt.xlabel(\"False positive rate\", fontsize = 15)\n",
    "plt.ylabel(\"True positive rate\", fontsize = 15)\n",
    "plt.title(\"ROC curve for BERT + 1D CNN\", fontsize = 20)\n",
    "plt.legend(loc=\"best\",  prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "\n",
    "precision_bert2_cnn1, recall_bert2_cnn1, _ = precision_recall_curve(ytest, bert2_probs_cnn1)\n",
    "auc_precision_recall_bert2_cnn1 = auc(recall_bert2_cnn1, precision_bert2_cnn1)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(recall_bert2_cnn1, precision_bert2_cnn1, label=\"BERT + 1D-CNN: {:.3f}\".format(auc_precision_recall_bert2_cnn1))\n",
    "plt.xlabel(\"Recall\", fontsize = 15)\n",
    "plt.ylabel(\"Precision\", fontsize = 15)\n",
    "plt.title(\"PR curve for BERT + 1D CNN\", fontsize = 20)\n",
    "plt.legend(loc=\"best\",  prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TITm7tMnRkv7"
   },
   "source": [
    "# 2D CNN Without Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-ItbbUTyZ8Q"
   },
   "outputs": [],
   "source": [
    "################ Without tuner ########################\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Activation, MaxPooling1D, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import utils\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from kerastuner import RandomSearch\n",
    "from tensorflow.keras import utils\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.20, random_state=None, shuffle=True)\n",
    "xval, xtest, yval, ytest = train_test_split(xtest, ytest, test_size = 0.5, random_state=None, shuffle=True)\n",
    "\n",
    "def CNN_2D():\n",
    "    model = Sequential()\n",
    "\n",
    "    # layer 1\n",
    "    model.add(Conv2D(32, 4, 4, input_shape=(768, 23, 1), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(2))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "    # layer 2\n",
    "    model.add(Conv2D(16, 2, 2, activation=\"relu\"))\n",
    "    #model.add(MaxPooling2D(2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "    # Flattening Layer:\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(96, activation=\"relu\"))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "\n",
    "    # Last Layer:\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr = 0.00002),\n",
    "                  metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    return model\n",
    "\n",
    "model2 = CNN_2D()\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.0008, patience=40,  verbose=1, mode='min')\n",
    "\n",
    "history2_ = model2.fit(np.asarray(xtrain).reshape(len(np.asarray(xtrain)),768,23,1), utils.to_categorical(ytrain,2),\n",
    "                    validation_data=(np.asarray(xval).reshape(len(np.asarray(xval)),768,23,1), utils.to_categorical(yval,2)), callbacks = [stop_early],\n",
    "                    epochs=200, batch_size=150, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7K1s35JDbq_v"
   },
   "outputs": [],
   "source": [
    "model2.evaluate(np.asarray(xtest).reshape(len(np.asarray(xtest)),768,23,1), utils.to_categorical(ytest,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbikXZamiNuz"
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(history2_.history[\"accuracy\"])\n",
    "plt.plot(history2_.history[\"val_accuracy\"])\n",
    "plt.xlabel(\"Epoch\", fontsize = 15)\n",
    "plt.ylabel(\"Accuracy\", fontsize = 15)\n",
    "plt.title(\"2D CNN Model Accuracy\", fontsize = 20)\n",
    "plt.legend([\"Train\", \"Test\"], loc=\"upper left\",  prop={\"size\": 15})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(history2_.history[\"loss\"])\n",
    "plt.plot(history2_.history[\"val_loss\"])\n",
    "plt.xlabel(\"Epoch\", fontsize = 15)\n",
    "plt.ylabel(\"Loss\", fontsize = 15)\n",
    "plt.title(\"2D CNN Model Loss\", fontsize = 20)\n",
    "plt.legend([\"Train\", \"Test\"], loc=\"upper left\",  prop={\"size\": 15})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "plt.clf()\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnn_predictions3 = model2.predict(np.asarray(xtest).reshape(len(np.asarray(xtest)),768,23,1))\n",
    "bert2_probs = cnn_predictions3[:,1]\n",
    "print(cnn_predictions3[:,1])\n",
    "cnn_predictions3 = np.argmax(cnn_predictions3, axis = 1)\n",
    "print(cnn_predictions3)\n",
    "confusion_matrix = confusion_matrix(ytest, cnn_predictions3)\n",
    "sns.heatmap(confusion_matrix, annot = True, fmt = \"d\", cbar = False)\n",
    "plt.title(\"BERT + 2D CNN + H. Tuning Confusion Matrix\", fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_keras_bert_cnn2_hyp, tpr_keras_bert_cnn2_hyp, thresholds_keras = roc_curve(ytest, bert2_probs)\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "auc_keras_bert_cnn2_hyp = auc(fpr_keras_bert_cnn2_hyp, tpr_keras_bert_cnn2_hyp)\n",
    "print(\"AUC Score\", auc_keras_bert_cnn2_hyp)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras_bert_cnn2_hyp, tpr_keras_bert_cnn2_hyp, label=\"BERT + 2D-CNN: {:.3f}\".format(auc_keras_bert_cnn2_hyp))\n",
    "plt.xlabel(\"False positive rate\", fontsize = 15)\n",
    "plt.ylabel(\"True positive rate\", fontsize = 15)\n",
    "plt.title(\"ROC curve for BERT + 2D CNN\", fontsize = 20)\n",
    "plt.legend(loc=\"best\",  prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "precision_bert2, recall_bert2, _ = precision_recall_curve(ytest, bert2_probs)\n",
    "auc_precision_recall_bert2 = auc(recall_bert2, precision_bert2)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(recall_bert2, precision_bert2, label=\"BERT + 2D-CNN: {:.3f}\".format(auc_precision_recall_bert2))\n",
    "plt.xlabel(\"Recall\", fontsize = 15)\n",
    "plt.ylabel(\"Precision\", fontsize = 15)\n",
    "plt.title(\"PR curve for BERT + 2D CNN\", fontsize = 20)\n",
    "plt.legend(loc=\"best\",  prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dV8ypFSNRtxn"
   },
   "source": [
    "# 2D CNN with Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-cqr8VMCucQ"
   },
   "outputs": [],
   "source": [
    "######################################## BERT + 2D CNN PART ############################################\n",
    "\n",
    "def build_model(hp):\n",
    "    # create model object\n",
    "    model = keras.Sequential([\n",
    "\n",
    "        # adding first convolutional layer\n",
    "        keras.layers.Conv2D(\n",
    "            # adding filter\n",
    "            filters=hp.Int('conv_1_filter', min_value=8, max_value=32, step=2),\n",
    "            # adding filter size or kernel size\n",
    "            kernel_size=hp.Choice('conv_1_kernel', values=[2, 6]),\n",
    "            # activation function\n",
    "            activation='relu',\n",
    "            input_shape=(768, 23, 1)),\n",
    "\n",
    "        # adding second convolutional layer\n",
    "        keras.layers.Conv2D(\n",
    "            # adding filter\n",
    "            filters=hp.Int('conv_2_filter', min_value=16, max_value=32, step=4),\n",
    "            # adding filter size or kernel size\n",
    "            kernel_size=hp.Choice('conv_2_kernel', values=[2, 8]),\n",
    "            # activation function\n",
    "            activation='relu'\n",
    "\n",
    "        ),\n",
    "\n",
    "        # adding flatten layer\n",
    "        keras.layers.Flatten(),\n",
    "        # adding dense layer\n",
    "        keras.layers.Dense(\n",
    "            units=hp.Int('dense_1_units', min_value=16, max_value=96, step=4),\n",
    "            activation='relu'\n",
    "        ),\n",
    "\n",
    "        # output layer\n",
    "        keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # compilation of model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[0.00006, 0.00008, 0.0001])),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#importing random search\n",
    "from kerastuner import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "#creating randomsearch object\n",
    "tuner = RandomSearch(build_model, objective='val_accuracy', max_trials = 20, directory = \"output\", project_name = \"Gly_AfterHyperParameterTuning_2D_CNN\")\n",
    "\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.20, random_state=None, shuffle=True)\n",
    "xval, xtest, yval, ytest = train_test_split(xtest, ytest, test_size = 0.5, random_state=None, shuffle=True)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.0008, patience=40,  verbose=1, mode='min')\n",
    "\n",
    "tuner.search(np.asarray(xtrain).reshape(len(np.asarray(xtrain)),768,23,1), utils.to_categorical(ytrain,2),\n",
    "             validation_data = (np.asarray(xval.reshape(len(np.asarray(xval)),768,23,1)), utils.to_categorical(yval,2)), epochs = 30, callbacks = [stop_early])\n",
    "\n",
    "\n",
    "model_2D_ht = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "model_2D_ht.summary()\n",
    "\n",
    "history3 = model_2D_ht.fit(np.asarray(xtest).reshape(len(np.asarray(xtest)),768,23,1), utils.to_categorical(ytest,2),\n",
    "                           epochs=200, batch_size = 100, validation_split=0.1,\n",
    "                           initial_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49pLITNvrppr"
   },
   "outputs": [],
   "source": [
    "plt.plot(history3.history[\"accuracy\"])\n",
    "plt.plot(history3.history[\"val_accuracy\"])\n",
    "plt.xlabel(\"Epoch\", fontsize = 15)\n",
    "plt.ylabel(\"Accuracy\", fontsize = 15)\n",
    "plt.title(\"2D CNN Model Accuracy\", fontsize = 20)\n",
    "plt.legend([\"Train\", \"Test\"], loc=\"upper left\",  prop={\"size\": 15})\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10), dpi = 600)\n",
    "\n",
    "\n",
    "plt.plot(history3.history[\"loss\"])\n",
    "plt.plot(history3.history[\"val_loss\"])\n",
    "plt.xlabel(\"Epoch\", fontsize = 15)\n",
    "plt.ylabel(\"Loss\", fontsize = 15)\n",
    "plt.title(\"2D CNN Model Loss\", fontsize = 20)\n",
    "plt.legend([\"Train\", \"Test\"], loc=\"upper left\",  prop={\"size\": 15})\n",
    "plt.show()\n",
    "print(model_2D_ht.summary())\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnn_predictions3 = model_2D_ht.predict(np.asarray(xtest).reshape(len(np.asarray(xtest)),768,23,1))\n",
    "bert2_probs = cnn_predictions3[:,1]\n",
    "print(cnn_predictions3[:,1])\n",
    "cnn_predictions3 = np.argmax(cnn_predictions3, axis = 1)\n",
    "print(cnn_predictions3)\n",
    "confusion_matrix = confusion_matrix(ytest, cnn_predictions3)\n",
    "sns.heatmap(confusion_matrix, annot = True, fmt = \"d\", cbar = False)\n",
    "plt.title(\"BERT + 2D CNN + H. Tuning Confusion Matrix\", fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "print(cnn_predictions3)\n",
    "fpr_keras_bert_cnn2_hyp, tpr_keras_bert_cnn2_hyp, thresholds_keras = roc_curve(ytest, bert2_probs)\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "auc_keras_bert_cnn2_hyp = auc(fpr_keras_bert_cnn2_hyp, tpr_keras_bert_cnn2_hyp)\n",
    "print(\"AUC Score\", auc_keras_bert_cnn2_hyp)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras_bert_cnn2_hyp, tpr_keras_bert_cnn2_hyp, label=\"BERT + 2D-CNN: {:.3f}\".format(auc_keras_bert_cnn2_hyp))\n",
    "plt.xlabel(\"False positive rate\", fontsize = 15)\n",
    "plt.ylabel(\"True positive rate\", fontsize = 15)\n",
    "plt.title(\"ROC curve for BERT + 2D CNN\", fontsize = 20)\n",
    "plt.legend(loc=\"best\",  prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "\n",
    "precision_bert2, recall_bert2, _ = precision_recall_curve(ytest, bert2_probs)\n",
    "auc_precision_recall_bert2 = auc(recall_bert2, precision_bert2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(recall_bert2, precision_bert2, label=\"BERT + 2D-CNN: {:.3f}\".format(auc_precision_recall_bert2))\n",
    "plt.xlabel(\"Recall\", fontsize = 15)\n",
    "plt.ylabel(\"Precision\", fontsize = 15)\n",
    "plt.title(\"PR curve for BERT + 2D CNN\", fontsize = 20)\n",
    "plt.legend(loc=\"best\",  prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpLZXj4SRysv"
   },
   "source": [
    "# ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jA9T5RUmSRsN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv1D, ZeroPadding1D, MaxPooling1D, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import utils\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from kerastuner import RandomSearch\n",
    "from tensorflow.keras import utils\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_addons as tfa\n",
    "import pickle\n",
    "import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "input_shape = (768, 23, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdPgGob9SYyM"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 200\n",
    "num_epochs = 50\n",
    "image_size = 16\n",
    "patch_size = 6\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 12\n",
    "transformer_units = [projection_dim * 2, projection_dim, ]\n",
    "transformer_layers =  1\n",
    "mlp_head_units = [2048, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVE4r0znSdxz"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "#####  Use data augmentation\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(np.asarray(xtrain).reshape(len(np.asarray(xtrain)),768,23,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4T2pZxkSjEv"
   },
   "outputs": [],
   "source": [
    "#######  Implement multilayer perceptron (MLP)\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvEngQ_FSjHv"
   },
   "outputs": [],
   "source": [
    "#######  Implement patch creation as a layer\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INpW1JmgSjKg"
   },
   "outputs": [],
   "source": [
    "######  Implement the patch encoding layer\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvP7mawcSjNS"
   },
   "outputs": [],
   "source": [
    "# ################################ EDITTING VIT CODE #################################\n",
    "\n",
    "######  Build the ViT model\n",
    "\n",
    "def optimal_vit_classifier(hp):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    print(augmented.shape)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads = num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes, activation =\"softmax\")(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate = learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[0.0005, 0.0001])),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC(),\n",
    "                           tf.keras.metrics.AUC(curve = \"ROC\"), tf.keras.metrics.AUC(curve = \"PR\")])\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner_ch = kt.RandomSearch(optimal_vit_classifier,\n",
    "                        objective=\"val_accuracy\",\n",
    "                        max_trials = 15, directory = \"output\", project_name = \"Gly_AfterHyperParameterTuning_ViT_homo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3Za5TdsSjST"
   },
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.0008, patience=100,  verbose=1, mode='min')\n",
    "\n",
    "tuner_ch.search(np.asarray(xtrain.reshape(len(np.asarray(xtrain)),768,23,1)), utils.to_categorical(ytrain,2),\n",
    "                validation_data = (np.asarray(xval.reshape(len(np.asarray(xval)),768,23,1)), utils.to_categorical(yval,2)), epochs = 50, callbacks = [stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzC7YL7BSjha"
   },
   "outputs": [],
   "source": [
    "model3 = tuner_ch.get_best_models(num_models=1)[0]\n",
    "history3_ = model3.fit(np.asarray(xtest).reshape(len(np.asarray(xtest)),768,23,1), utils.to_categorical(ytest,2), epochs = 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDYagx3uSjpu"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history3_.history)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
